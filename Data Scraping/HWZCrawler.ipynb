{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class HardwarezoneCrawler:\n",
    "    def __init__(self) -> None:\n",
    "        options = webdriver.ChromeOptions() \n",
    "        options.add_argument('log-level=3')\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"enable-automation\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-browser-side-navigation\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        self.driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    def get_options():\n",
    "        options = webdriver.ChromeOptions() \n",
    "        options.add_argument('log-level=3')\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"enable-automation\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-browser-side-navigation\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        return options\n",
    "        \n",
    "    def get_url(self, aspect_item: str):\n",
    "        self.driver.quit()\n",
    "        options = webdriver.ChromeOptions() \n",
    "        options.add_argument('log-level=3')\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"enable-automation\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-browser-side-navigation\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        self.driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "        self.driver.get(\"https://www.hardwarezone.com.sg/search/forum\")\n",
    "\n",
    "        # Wait for the search input field to be clickable\n",
    "        search_navi = WebDriverWait(self.driver, 11).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//form/input[@class='text']\"))\n",
    "        )\n",
    "        search_navi.send_keys(aspect_item)\n",
    "        time.sleep(10)\n",
    "\n",
    "        submit_btn = self.driver.find_element(\"xpath\",\"//form/input[@type='submit']\")\n",
    "        webdriver.ActionChains(self.driver).move_to_element(submit_btn).click(submit_btn).perform()\n",
    "        time.sleep(10)\n",
    "        forum_btn = self.driver.find_element(\"xpath\",\"//ul[@class='tabs-menu']/li[6]\")\n",
    "        webdriver.ActionChains(self.driver).move_to_element(forum_btn).click(forum_btn).perform()\n",
    "        time.sleep(10)\n",
    "\n",
    "        self.driver.execute_script (\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "\n",
    "        # The first page of forums\n",
    "        threads_list = []\n",
    "        links_list = self.driver.find_elements(\"xpath\",\"//a[@class='gs-title']\")\n",
    "        for link in links_list:\n",
    "            if link.get_attribute('href'):\n",
    "                threads_list.append(link.get_attribute('href'))\n",
    "                \n",
    "        # The rest pages of forums\n",
    "        for i in range(2, 11):\n",
    "            try:\n",
    "                page_ele = self.driver.find_element(\"xpath\", \"//div[@aria-label='Page \" + str(i) + \"']\")\n",
    "                webdriver.ActionChains(self.driver).move_to_element(page_ele).click(page_ele).perform()\n",
    "                time.sleep(10)\n",
    "            except NoSuchElementException:\n",
    "                print(\"Page element not found for page\", i)\n",
    "                a=input()\n",
    "            \n",
    "            links_list = self.driver.find_elements(\"xpath\",\"//a[@class='gs-title']\")\n",
    "            for link in links_list:\n",
    "                if link.get_attribute('href'):\n",
    "                    threads_list.append(link.get_attribute('href'))\n",
    "\n",
    "        # Navigate to Google\n",
    "        try:\n",
    "            google_navi_ele = self.driver.find_element(\"xpath\",\"//div[@class='gcsc-more-maybe-branding-root']/a\")\n",
    "            google_search_url = google_navi_ele.get_attribute(\"href\")\n",
    "            self.driver.get(google_search_url)\n",
    "        except NoSuchElementException:\n",
    "            print(\"Page element not found for page\")\n",
    "\n",
    "        # The first page of google search results\n",
    "        links_list = self.driver.find_elements(\"xpath\",\"//div[@class='yuRUbf']/a\")\n",
    "        for link in links_list:\n",
    "            if link.get_attribute(\"href\"):\n",
    "                threads_list.append(link.get_attribute(\"href\"))\n",
    "\n",
    "        self.driver.execute_script (\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "        # The rest pages of google search results\n",
    "        for i in range(2, 16):\n",
    "            try:\n",
    "                page_ele = self.driver.find_element(\"xpath\",\"//a[@aria-label='Page \" + str(i) + \"']\")\n",
    "                webdriver.ActionChains(self.driver).move_to_element(page_ele).click(page_ele).perform()\n",
    "                time.sleep(10)\n",
    "                \n",
    "                links_list = self.driver.find_elements(\"xpath\",\"//div[@class='yuRUbf']/a\")\n",
    "                for link in links_list:\n",
    "                    if link.get_attribute('href'):\n",
    "                        threads_list.append(link.get_attribute('href'))\n",
    "            except:\n",
    "                print(\"We can not find the pages that required, end the loop\")\n",
    "                    \n",
    "        threads_list = list(set(threads_list))\n",
    "        aspects_list = [aspect_item for i in range(len(threads_list))]\n",
    "        data_df = pd.DataFrame(list(zip(threads_list, aspects_list)), columns=['url', 'aspect'])\n",
    "        data_df.to_csv(\"E:/Critical Inquiry/Code/Data Scraping/data/hwz_data_\" + aspect_item + \".csv\", index=False)\n",
    "    \n",
    "    def getReviews_content(self, csv_path: str, aspect_term: str):\n",
    "        url_df = pd.read_csv(csv_path)\n",
    "        url_df = url_df.drop(url_df[url_df['url'] == \"https://forums.hardwarezone.com.sg/forums/eat-drink-man-woman.16/\"].index)\n",
    "        \n",
    "        aspects_list = []\n",
    "        urls_list = []\n",
    "        titles_list = []\n",
    "        contents_list = []\n",
    "        authors_list = []\n",
    "        timeStamps_list = []\n",
    "        memberships_list = []\n",
    "\n",
    "        for index, row in tqdm(url_df.iterrows()):\n",
    "            if row.aspect != aspect_term:\n",
    "                continue\n",
    "            self.driver.quit()\n",
    "            options = webdriver.ChromeOptions() \n",
    "            options.add_argument('log-level=3')\n",
    "            options.add_argument(\"start-maximized\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"enable-automation\")\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-browser-side-navigation\")\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "            self.driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "            self.driver.get(row.url)\n",
    "            try:\n",
    "                thread_first_page = self.driver.find_element(\"xpath\",\"//ul[@class='pageNav-main']/li/a\")\n",
    "                self.driver.get(thread_first_page.get_attribute(\"href\"))\n",
    "                # time.sleep(1)\n",
    "            except:\n",
    "                print(self.driver.find_element(\"xpath\",\"//h1[@class='p-title-value']\").text, \"\\t, this thread Only one pages\")\n",
    "\n",
    "            contents_web = self.driver.find_elements(\"xpath\",\"//div[@class='bbWrapper']\")\n",
    "            for content in contents_web:\n",
    "                contents_list.append(content.text)\n",
    "                titles_list.append(self.driver.find_element(\"xpath\",\"//h1[@class='p-title-value']\").text)\n",
    "                urls_list.append(row.url)\n",
    "                aspects_list.append(row.aspect)\n",
    "\n",
    "            authors_web = self.driver.find_elements(\"xpath\",\"//a[@class='username ']\")\n",
    "            for author in authors_web:\n",
    "                authors_list.append(author.text)\n",
    "\n",
    "            timeStamps_web = self.driver.find_elements(\"xpath\",\"//time[@class='u-dt']\")\n",
    "            for timeStamp in timeStamps_web:\n",
    "                timeStamps_list.append(timeStamp.text)\n",
    "\n",
    "            memberships_web = self.driver.find_elements(\"xpath\",\"//h5[@class='userTitle message-userTitle']\")\n",
    "            for membership in memberships_web:\n",
    "                memberships_list.append(membership.text)\n",
    "                \n",
    "            flag = True\n",
    "            cnt = 0\n",
    "            while flag and cnt <= 100:\n",
    "                try:\n",
    "                    forum_first_page = self.driver.find_element(\"xpath\",\"//a[@class='pageNav-jump pageNav-jump--next']\")\n",
    "                    self.driver.get(forum_first_page.get_attribute(\"href\"))\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                    contents_web = self.driver.find_elements(\"xpath\",\"//div[@class='bbWrapper']\")\n",
    "                    for content in contents_web:\n",
    "                        contents_list.append(content.text)\n",
    "                        titles_list.append(self.driver.find_element(\"xpath\",\"//h1[@class='p-title-value']\").text)\n",
    "                        urls_list.append(row.url)\n",
    "                        aspects_list.append(row.aspect)\n",
    "\n",
    "                    authors_web = self.driver.find_elements(\"xpath\",\"//a[@class='username ']\")\n",
    "                    for author in authors_web:\n",
    "                        authors_list.append(author.text)\n",
    "\n",
    "                    timeStamps_web = self.driver.find_elements(\"xpath\",\"//time[@class='u-dt']\")\n",
    "                    for timeStamp in timeStamps_web:\n",
    "                        timeStamps_list.append(timeStamp.text)\n",
    "\n",
    "                    memberships_web = self.driver.find_elements(\"xpath\",\"//h5[@class='userTitle message-userTitle']\")\n",
    "                    for membership in memberships_web:\n",
    "                        memberships_list.append(membership.text)\n",
    "                except: \n",
    "                    flag = False\n",
    "                    cnt = 0\n",
    "                    print(\"Websites Name:\\t\", self.driver.find_element(\"xpath\",\"//h1[@class='p-title-value']\").text, ', We encounter a problem in this websites or we come the end the this thread')\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "        hardware_zone_df = pd.DataFrame(list(zip(urls_list, titles_list, contents_list,\n",
    "                                                        authors_list, timeStamps_list, memberships_list, aspects_list)),\n",
    "                                            columns=['url', 'title', 'content', 'author', \n",
    "                                                        'time_stamp', 'membership', \"aspect\"])\n",
    "        hardware_zone_df.to_csv(\"E:/Critical Inquiry/Code/Data Scraping/data/hwz_thread_all_data_\" + aspect_term + '.csv', index=False)\n",
    "    \n",
    "    def write_to_disk(self, aspects_list, urls_list, titles_list, contents_list, authors_list, timeStamps_list, memberships_list, aspect_term):\n",
    "        hardware_zone_df = pd.DataFrame({\n",
    "            'url': urls_list,\n",
    "            'title': titles_list,\n",
    "            'content': contents_list,\n",
    "            'author': authors_list,\n",
    "            'time_stamp': timeStamps_list,\n",
    "            'membership': memberships_list,\n",
    "            'aspect': aspects_list\n",
    "        })\n",
    "        # Write data to disk\n",
    "        file_path = f\"E:/Critical Inquiry/Code/Data Scraping/data/hwz_thread_all_data_{aspect_term}.csv\"\n",
    "        if os.path.exists(file_path):\n",
    "            mode = 'a'\n",
    "            header = False\n",
    "        else:\n",
    "            mode = 'w'\n",
    "            header = True\n",
    "        hardware_zone_df.to_csv(file_path, mode=mode, header=header, index=False)\n",
    "    \n",
    "    def search_items(self, aspect_items: list):\n",
    "        for item in aspect_items:\n",
    "            self.getAndSave_url(item)\n",
    "        \n",
    "        self.combine_data()\n",
    "    \n",
    "    def combine_url_data(self):\n",
    "        all_data = glob.glob(os.path.join(\"E:/Critical Inquiry/Code/Data Scraping/data/\", \"hwz_data_*.csv\"))\n",
    "        df_from_each_file = (pd.read_csv(f, sep=',') for f in all_data)\n",
    "        df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
    "        df_merged.drop_duplicates()\n",
    "        \n",
    "        # Drop noise pages\n",
    "        for index, row in tqdm(df_merged.iterrows()):\n",
    "            cnt = 1\n",
    "            self.driver.get(row.url)\n",
    "            try:\n",
    "                pages = self.driver.find_elements(\"xpath\",\"//li[@class='pageNav-page ']/a\")\n",
    "                for page in pages: \n",
    "                    cnt = max(int(page.text), cnt)\n",
    "            except:\n",
    "                cnt = 1\n",
    "                \n",
    "            if cnt >= 100:\n",
    "                df_merged.drop(index, inplace=True)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        df_merged.to_csv(\"E:/Critical Inquiry/Code/Data Scraping/data/hwz_url_data.csv\", index=False)\n",
    "        \n",
    "    def combine_data(self):\n",
    "        all_data = glob.glob(os.path.join(\"E:/Critical Inquiry/Code/Data Scraping/data/\", \"hwz_thread_all_data_*.csv\"))\n",
    "        df_from_each_file = (pd.read_csv(f, sep=',') for f in all_data)\n",
    "        df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
    "        df_merged.drop_duplicates(subset=['content'], inplace=True)\n",
    "        \n",
    "        df_merged.to_csv(\"E:/Critical Inquiry/Code/Data Scraping/data/hwz_all_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "Page element not found for page\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"lgbtq\", \"lgbt\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"gay\", \"lesbian\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"bisexual\", \"transgender\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 8\n",
      "Page element not found for page 9\n",
      "Page element not found for page 10\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "Page element not found for page\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"queer\", \"pink dot\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"pride month\", \"section 377A\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page element not found for page 11\n",
      "Page element not found for page 12\n",
      "Page element not found for page 13\n",
      "Page element not found for page 14\n",
      "Page element not found for page 15\n",
      "Page element not found for page 16\n",
      "Page element not found for page 17\n",
      "Page element not found for page 18\n",
      "Page element not found for page 19\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n",
      "We can not find the pages that required, end the loop\n"
     ]
    }
   ],
   "source": [
    "aspect_items = [\"same-sex marriage\"]\n",
    "#aspect_items = [\"lgbtq\"]\n",
    "crawler = HardwarezoneCrawler()\n",
    "for aspect in aspect_items:\n",
    "    crawler.get_url(aspect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
