{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import pandas as pd\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"D8uhE8xekateXsNSjds3_Q\",\n",
    "    client_secret=\"RcAkITSHFpjagUK8GNwoHbiq95ko6g\", \n",
    "    user_agent=\"Scraper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore\n"
     ]
    }
   ],
   "source": [
    "sg_subreddit = reddit.subreddit(\"Singapore\")\n",
    "print(sg_subreddit.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_headers = ['author', 'created_utc', 'id', \n",
    "                      'is_original_content', 'is_self', \n",
    "                      'link_flair_text', 'locked', 'name', \n",
    "                      'num_comments', 'over_18', 'permalink', \n",
    "                      'score', 'selftext', 'spoiler', 'stickied', \n",
    "                      'subreddit', 'title', 'upvote_ratio', 'url']\n",
    "\n",
    "\n",
    "sub_headers = ['title', 'created_utc', 'id' ,'score','num_comments', 'upvote_ratio', 'permalink']\n",
    "\n",
    "search_terms = [\"lgbtq\", \"lgbt\", \"gay\", \"lesbian\", \"bisexual\", \"transgender\", \"queer\", \"pink dot\", \"pride month\", \"section 377A\", \"same-sex marriage\", 'homosexual','sexual orientation', 'tranny', 'ladyboy', 'trans', 'anti-gay law', '377a penal code']\n",
    "\n",
    "with open(\"reddit_test_submission_db.csv\", 'a', \n",
    "          encoding=\"utf-8\", newline='') as f_object:\n",
    "    for term in search_terms: \n",
    "        posts = sg_subreddit.search(term)\n",
    "        for post in posts:\n",
    "            if post.score > 50: \n",
    "        # fields requested thourgh PRAW\n",
    "                data = {'title': post.title, 'score': post.score, 'num_comments': post.num_comments,\n",
    "                        'created_utc': post.created_utc, 'id': post.id ,\n",
    "                        'upvote_ratio': post.upvote_ratio, 'permalink': post.permalink, \n",
    "                        }\n",
    "                dictwriter_object = csv.DictWriter(\n",
    "                    f_object, fieldnames=sub_headers)\n",
    "                dictwriter_object.writerow(data)\n",
    "    f_object.close()\n",
    "    \n",
    "# Delete duplicates\n",
    "post_db = pd.read_csv(\"reddit_test_submission_db.csv\", \n",
    "                      names=sub_headers, header=0)\n",
    "post_db.drop_duplicates(subset=\"permalink\", \n",
    "                        keep=\"last\", inplace=True)\n",
    "post_db.to_csv(\"reddit_test_submission_db.csv\", \n",
    "               index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submissions\n",
    "comment_headers = ['author', 'body', 'created_utc', \n",
    "                   'distinguished', 'edited', 'id', \n",
    "                   'is_submitter', 'link_id', 'parent_id', \n",
    "                   'permalink', 'saved', 'score', 'stickied', \n",
    "                   'submission', 'subreddit', 'subreddit_id']\n",
    "with open('reddit_test_comment_db.csv', 'a') as comment_file:\n",
    "    comments_db = pd.read_csv('reddit_test_comment_db.csv', \n",
    "                              usecols=[\"submission\"], \n",
    "                              names=comment_headers)\n",
    "    comment_file.close()\n",
    "submission_db = pd.read_csv(\"reddit_test_submission_db.csv\", \n",
    "                            usecols=[\"created_utc\", \"id\"])\n",
    "\n",
    "comments_set = set(comments_db[\"submission\"])\n",
    "# 259,200 is three days worth of seconds\n",
    "try:\n",
    "    time_cutoff = max([created_utc for post_id, created_utc in \n",
    "         zip(submission_db.id, submission_db.created_utc) \n",
    "         if post_id in comments_set]) - 259200\n",
    "except:\n",
    "    time_cutoff = submission_db[\"created_utc\"].min()\n",
    "submissions_to_pull = submission_db.loc[submission_db\n",
    "                                        [\"created_utc\"] >= \n",
    "                                        time_cutoff, \"id\"]\n",
    "#This block pulls all comments for the list of submissions we have identified\n",
    "with open(\"reddit_test_comment_DB.csv\", 'a', \n",
    "          encoding=\"utf-8\", newline='') as f_object:\n",
    "    for row in submissions_to_pull:\n",
    "        submission = reddit.submission(id=row)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            data = {'author': comment.author, 'body': \n",
    "                    comment.body, 'created_utc': \n",
    "                    comment.created_utc, 'distinguished': \n",
    "                    comment.distinguished, 'edited': \n",
    "                    comment.edited, 'id': comment.id, \n",
    "                    'is_submitter': comment.is_submitter, \n",
    "                    'link_id': comment.link_id, 'parent_id': \n",
    "                    comment.parent_id, 'permalink': \n",
    "                    comment.permalink, 'saved': comment.saved, \n",
    "                    'score': comment.score, 'stickied': \n",
    "                    comment.stickied, 'submission': \n",
    "                    comment.submission, 'subreddit': \n",
    "                    comment.subreddit, \n",
    "                    'subreddit_id': comment.subreddit_id}\n",
    "            dictwriter_object = csv.DictWriter(f_object, \n",
    "                                               fieldnames=\n",
    "                                               comment_headers)\n",
    "            dictwriter_object.writerow(data)\n",
    "    f_object.close()\n",
    "    \n",
    "# drop duplicate rows\n",
    "comment_db = pd.read_csv(\"reddit_test_comment_DB.csv\", \n",
    "                         names=comment_headers, header=0)\n",
    "\n",
    "comment_db.drop_duplicates(subset=[\"permalink\", \"body\", \"id\"],\n",
    "                           keep = \"last\", inplace=True)\n",
    "\n",
    "comment_db.drop_duplicates(subset=\"permalink\", \n",
    "                           keep = \"first\", inplace=True)\n",
    "comment_db.to_csv(\"reddit_test_comment_DB.csv\", \n",
    "                  index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "capturedData = pd.read_csv('reddit_test_comment_DB.csv')\n",
    "capturedData.to_csv('capturedData23.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
